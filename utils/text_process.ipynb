{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文献数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text document saved as maintext_clean_test/output/A_Facile_and_General_Approach_for_the_Direct_Fabri_maintext_cleaned.txt\n",
      "Cleaned text document saved as maintext_clean_test/output/Atomically_Dispersed_Transition_Metals_on_Carbon_N_maintext_cleaned.txt\n",
      "Cleaned text document saved as maintext_clean_test/output/Enhanced_oxygen_reduction_reaction_activity_of_nit_maintext_cleaned.txt\n",
      "Cleaned text document saved as maintext_clean_test/output/A_KClassisted_pyrolysis_strategy_to_fabricate_nitr_maintext_cleaned.txt\n",
      "Cleaned text document saved as maintext_clean_test/output/Electrocatalytic_activity_of_nitrogen_doped_carbon_maintext_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def clean_document(input_data):\n",
    "    \"\"\"\n",
    "    Cleans the input document by removing references, acknowledgements,\n",
    "    and in-text citations while preserving measurements.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data (str or dict): The input document content as a string (plain text)\n",
    "      or a dictionary (JSON-like structure).\n",
    "\n",
    "    Returns:\n",
    "    - str or dict: The cleaned document in the same format as the input.\n",
    "    \"\"\"\n",
    "    def clean_text(text):\n",
    "        # Define patterns for non-content sections\n",
    "        non_content_sections = [\n",
    "            'References',\n",
    "            'NOTES AND REFERENCES',\n",
    "            'Notes and references',\n",
    "            'REFERENCES',\n",
    "            '参考文献',\n",
    "            '引用',\n",
    "            'Bibliography',\n",
    "            'Cited By',\n",
    "            'Acknowledgment',\n",
    "            'Acknowledgement',\n",
    "            'Acknowledgments',\n",
    "            'Acknowledgements',\n",
    "            'Supporting Information',\n",
    "            'Author Information',\n",
    "            'Terms & Conditions',\n",
    "            'This article is cited by',\n",
    "            'Conflicts of interest',\n",
    "            # Add any other headings that mark the end of the main content\n",
    "        ]\n",
    "        # Build a combined pattern for non-content sections\n",
    "        combined_non_content_pattern = '|'.join([re.escape(section) for section in non_content_sections])\n",
    "\n",
    "        # Reverse the text to search from the end\n",
    "        reversed_text = text[::-1]\n",
    "        # Reverse the patterns as well\n",
    "        reversed_patterns = [section[::-1] for section in non_content_sections]\n",
    "        combined_reversed_pattern = '|'.join([re.escape(pattern) for pattern in reversed_patterns])\n",
    "\n",
    "        # Find the earliest occurrence from the end (which is the last occurrence in the original text)\n",
    "        match = re.search(combined_reversed_pattern, reversed_text, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            # Truncate the text from the position where the non-content section starts\n",
    "            cut_off_position = len(text) - match.end()\n",
    "            text = text[:cut_off_position]\n",
    "\n",
    "        # Now remove in-text citations like [12], (12), or superscript numbers\n",
    "        # First, protect measurements to prevent accidental removal\n",
    "        units = [\n",
    "            '°C', 'h', 'mV', 'V', 'wt%', 'μm', 'nm', 'g', 'mg', 'kg', 'cm−2',\n",
    "            'cm²', 'mA', 'wt%', 'kPa', 'MPa', 'kW', 's', 'min', 'hours', 'days',\n",
    "            'weeks', 'months', 'years', 'bar', 'mol', 'atm', 'rpm', 'wt. %', '%',\n",
    "            'mm', 'A', 'K', 'J', 'Pa', 'mbar', 'mTorr', 'μA', 'nA', 'pA', 'Torr',\n",
    "            # Add any additional units as needed\n",
    "        ]\n",
    "        units_pattern = r'(?<!\\w)(\\d+(?:\\.\\d+)?\\s*(?:' + '|'.join(units) + r'))(?!\\w)'\n",
    "\n",
    "        # Placeholder for measurements to protect them during cleaning\n",
    "        measurement_placeholder = 'MEASUREMENTPLACEHOLDER'\n",
    "\n",
    "        def protect_measurements(text):\n",
    "            # Find all measurements to protect them\n",
    "            measurements = re.findall(units_pattern, text)\n",
    "            # Replace measurements with placeholders\n",
    "            for measurement in measurements:\n",
    "                escaped_measurement = re.escape(measurement)\n",
    "                text = re.sub(escaped_measurement, measurement_placeholder, text, count=1)\n",
    "            return text, measurements\n",
    "\n",
    "        def restore_measurements(text, measurements):\n",
    "            # Restore measurements from placeholders\n",
    "            for measurement in measurements:\n",
    "                text = text.replace(measurement_placeholder, measurement, 1)\n",
    "            return text\n",
    "\n",
    "        # Protect measurements\n",
    "        text_protected, measurements = protect_measurements(text)\n",
    "\n",
    "        # Remove in-text citations\n",
    "        # Patterns:\n",
    "        # - [12], [12,13], [12–15]\n",
    "        # - (12), (12,13), (12–15)\n",
    "        # - Superscript numbers (if any)\n",
    "        # - Numbers at the end of sentences, possibly preceded by a comma\n",
    "\n",
    "        # Pattern for in-text citations in square brackets\n",
    "        square_bracket_citation_pattern = r'\\[\\s*\\d+(?:–\\d+)?(?:,\\s*\\d+(?:–\\d+)?)*\\s*\\]'\n",
    "\n",
    "        # Pattern for in-text citations in parentheses\n",
    "        parenthesis_citation_pattern = r'\\(\\s*\\d+(?:–\\d+)?(?:,\\s*\\d+(?:–\\d+)?)*\\s*\\)'\n",
    "\n",
    "        # Pattern for superscript numbers (e.g., citation markers)\n",
    "        superscript_citation_pattern = r'\\^\\s*\\d+(?:,\\s*\\d+)*'\n",
    "\n",
    "        # Remove the in-text citations\n",
    "        text_no_citations = re.sub(square_bracket_citation_pattern, '', text_protected)\n",
    "        text_no_citations = re.sub(parenthesis_citation_pattern, '', text_no_citations)\n",
    "        text_no_citations = re.sub(superscript_citation_pattern, '', text_no_citations)\n",
    "\n",
    "        # Remove numbers at the end of sentences that are likely citations\n",
    "        end_of_sentence_citation_pattern = r',?\\s*\\d+(?:,\\s*\\d+)*(?=\\s|\\.|,|;|:|\\))'\n",
    "        text_no_citations = re.sub(end_of_sentence_citation_pattern, '', text_no_citations)\n",
    "\n",
    "        # Restore measurements\n",
    "        text_cleaned = restore_measurements(text_no_citations, measurements)\n",
    "\n",
    "        # Clean up extra spaces and punctuation\n",
    "        text_cleaned = re.sub(r'\\s{2,}', ' ', text_cleaned)\n",
    "        text_cleaned = re.sub(r'\\s+([.,;:])', r'\\1', text_cleaned)\n",
    "        text_cleaned = re.sub(r'\\(\\s*\\)', '', text_cleaned)  # Remove empty parentheses\n",
    "        text_cleaned = re.sub(r'\\[\\s*\\]', '', text_cleaned)  # Remove empty brackets\n",
    "\n",
    "        return text_cleaned.strip()\n",
    "\n",
    "    if isinstance(input_data, dict):\n",
    "        # Input is a JSON-like dictionary\n",
    "        # Remove non-content sections (case-insensitive)\n",
    "        non_content_keys = [\n",
    "            'Notes and references',\n",
    "            'REFERENCES',\n",
    "            'References',\n",
    "            #'Supporting Information',\n",
    "            'Acknowledgements',\n",
    "            'Acknowledgments',\n",
    "            'Conflicts of interest',\n",
    "            'Terms & Conditions',\n",
    "            # Add any other keys that are non-content\n",
    "        ]\n",
    "        # Create a set for faster lookup\n",
    "        non_content_keys_lower = set(key.lower() for key in non_content_keys)\n",
    "        keys_to_remove = [key for key in input_data.keys() if key.strip().lower() in non_content_keys_lower]\n",
    "        for key in keys_to_remove:\n",
    "            del input_data[key]\n",
    "\n",
    "        # Clean each section\n",
    "        for key in list(input_data.keys()):\n",
    "            content = input_data[key]\n",
    "            if isinstance(content, str):\n",
    "                cleaned_content = clean_text(content)\n",
    "                if not cleaned_content.strip():\n",
    "                    # Remove the section if it's empty after cleaning\n",
    "                    del input_data[key]\n",
    "                else:\n",
    "                    input_data[key] = cleaned_content\n",
    "            elif isinstance(content, list):\n",
    "                # If the content is a list of paragraphs\n",
    "                cleaned_paragraphs = [clean_text(paragraph) for paragraph in content]\n",
    "                # Remove empty paragraphs\n",
    "                cleaned_paragraphs = [p for p in cleaned_paragraphs if p.strip()]\n",
    "                if not cleaned_paragraphs:\n",
    "                    # Remove the section if all paragraphs are empty\n",
    "                    del input_data[key]\n",
    "                else:\n",
    "                    input_data[key] = cleaned_paragraphs\n",
    "            # Add more conditions if needed for other data types\n",
    "\n",
    "        return input_data\n",
    "\n",
    "    elif isinstance(input_data, str):\n",
    "        # Input is plain text\n",
    "        cleaned_text = clean_text(input_data)\n",
    "        return cleaned_text\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Input data must be either a string or a dictionary.\")\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Processes a file to clean it by removing references, acknowledgements,\n",
    "    and in-text citations while preserving measurements.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the input file.\n",
    "\n",
    "    The function saves the cleaned content to a new file with '_cleaned' appended to the original filename.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    try:\n",
    "        # Try to parse the content as JSON\n",
    "        data = json.loads(content)\n",
    "        # Input is JSON-like\n",
    "        cleaned_data = clean_document(data)\n",
    "        # Save the cleaned JSON\n",
    "        base, ext = os.path.splitext(file_path)\n",
    "        output_dir = os.path.join(os.path.dirname(file_path), 'output')\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "        new_file_path = os.path.join(output_dir, f\"{os.path.basename(base)}_cleaned{ext}\")\n",
    "        with open(new_file_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(cleaned_data, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Cleaned JSON document saved as {new_file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        # Input is plain text\n",
    "        cleaned_text = clean_document(content)\n",
    "        # Save the cleaned text\n",
    "        base, ext = os.path.splitext(file_path)\n",
    "        output_dir = os.path.join(os.path.dirname(file_path), 'output')\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "        new_file_path = os.path.join(output_dir, f\"{os.path.basename(base)}_cleaned{ext}\")\n",
    "        with open(new_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_text)\n",
    "        print(f\"Cleaned text document saved as {new_file_path}\")\n",
    "\n",
    "def batch_process_files(file_pattern):\n",
    "    \"\"\"\n",
    "    Batch processes multiple files matching the given pattern.\n",
    "\n",
    "    Parameters:\n",
    "    - file_pattern (str): The glob pattern to match files, e.g., 'documents/*.txt'\n",
    "    \"\"\"\n",
    "    for file_path in glob.glob(file_pattern):\n",
    "        process_file(file_path)\n",
    "\n",
    "# To process multiple files in a directory:\n",
    "batch_process_files('maintext_clean_test/*.txt')  # Adjust the pattern as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     45\u001b[0m     json_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 47\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_spectroscopy_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspectroscopy_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 将处理后的数据保存回 JSON 文件\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mprocess_spectroscopy_data\u001b[0;34m(json_data, spectroscopy_types)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prop \u001b[38;5;129;01min\u001b[39;00m material[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prop[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m spectroscopy_types:\n\u001b[0;32m---> 15\u001b[0m         combined_spectra[prop[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mprop\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     16\u001b[0m         combined_spectra[prop[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(prop[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munit\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m         combined_spectra[prop[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconditions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(prop[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconditions\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'value'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_spectroscopy_data(json_data, spectroscopy_types):\n",
    "    materials = json_data['materials']\n",
    "    new_materials = []\n",
    "\n",
    "    for material in materials:\n",
    "        # 用于存储各类谱图数据的字典\n",
    "        combined_spectra = {spec_type: defaultdict(list) for spec_type in spectroscopy_types}\n",
    "\n",
    "        # 遍历所有属性并收集指定类型的谱图数据\n",
    "        for prop in material['properties']:\n",
    "            if prop['type'] in spectroscopy_types:\n",
    "                combined_spectra[prop['type']]['values'].append(prop['value'])\n",
    "                combined_spectra[prop['type']]['units'].append(prop['unit'])\n",
    "                combined_spectra[prop['type']]['conditions'].append(prop['conditions'])\n",
    "                combined_spectra[prop['type']]['evidence'].append(prop['evidence'])\n",
    "\n",
    "        # 将收集到的谱图数据转换为合适的向量表示\n",
    "        for spec_type, data in combined_spectra.items():\n",
    "            if data['values']:\n",
    "                combined_vector = {\n",
    "                    \"type\": spec_type,\n",
    "                    \"values\": data['values'],\n",
    "                    \"units\": list(set(data['units'])),\n",
    "                    \"conditions\": list(set(data['conditions'])),\n",
    "                    \"evidence\": sum(data['evidence']) / len(data['evidence']) if data['evidence'] else None\n",
    "                }\n",
    "\n",
    "                # 将新生成的谱图数据添加回原材料属性中\n",
    "                material['properties'] = [prop for prop in material['properties'] if prop['type'] != spec_type]\n",
    "                material['properties'].append(combined_vector)\n",
    "\n",
    "        new_materials.append(material)\n",
    "\n",
    "    json_data['materials'] = new_materials\n",
    "    return json_data\n",
    "\n",
    "# 示例用法\n",
    "spectroscopy_types = [\"XPS\", \"XRD\", \"FTIR\", \"Raman\"]  # 添加你想要处理的其他谱图类型\n",
    "\n",
    "input_file = '/Users/yangz/Documents/projects/llm4catalyst/results/test.json'\n",
    "with open(input_file, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "processed_data = process_spectroscopy_data(json_data, spectroscopy_types)\n",
    "\n",
    "# 将处理后的数据保存回 JSON 文件\n",
    "with open(input_file, 'w') as f:\n",
    "    json.dump(processed_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Edge-sited_Fe-N4_atomic_species_Fe-N4_result_20240812062353.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Carbon_nanotubes_with_rich_pyridinic_nitrogen_CNT_result_20240812042632.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/3D_porous_Fe_N_C_electrocatalyst_Fe_N_C_result_20240811175645.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Graphitic-phase_C3N4_g-C3N4_result_20240812061308.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Nitrogen-doped_carbons_NCs_result_20240812075901.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/hetero-single-atom_ORR_electrocatalyst_Fe_Ni_h-SA_result_20240812054017.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/carbon_nanotubes_doped_with_nitrogen_CNT-N_result_20240811183729.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/N-doped_graphene_carbon_nanotube_composite_N-doped_graphene_CNT_result_20240812071337.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/bamboo-shaped_carbon_nitrogen_nanotubes_CNNTs_result_20240812042428.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Fe-N-C_N-OMC_Fe-N-C_N-OMC_result_20240811174725.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/cyclopentadienyliron_modified_reduced_graphene_oxide_rGFeCp_result_20240812063357.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Nitrogen-doped_multiwalled_carbon_nanotubes_N-CNTs_result_20240812075714.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Nitrogen-doped_graphene_NG_result_20240811175021.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/three-dimensional_graphene_nanoribbon_networks_doped_with_nitrogen_N-GRW_result_20240811171233.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/nitrogen-doped_bamboo-like_carbon_nanotubes_NBCNT_result_20240812031313.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Co3O4–x-carbon@Fe2–yCoyO3_heterostructural_hollow_polyhedrons_Co3O4–x-carbon@Fe2–yCoyO3_result_20240812045454.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/cobalt_nanoparticles_embedded_in_nitrogen_and_sulfur_co-doped_carbon_nanotubes_Co@NSCNT_result_20240812054249.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Fe-doped_Co3O4@C_nanoparticles_Fe-doped_Co3O4@C_result_20240812074306.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/N-doped_CNT_assembled_hollow_polyhedra_NCNHP_result_20240811180254.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/cobalt_oxide_doped_by_Ce_CoOx(Ce)_result_20240812025815.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/hierarchical_ZnxCo3–xO4_nanostructures_ZnxCo3–xO4_result_20240812080825.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/hierarchically_structured_porous_carbon_SA–Fe–HPC_result_20240811180952.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/ultrathin_2D_Co3O4_nanosheets_Co3O4_result_20240812072840.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/g-C3N4@N-G_g-C3N4@N-G_result_20240812080046.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/hierarchical_porous_heteroatom-doped_carbon_materials_Fe–N4_result_20240812074124.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Nitrogen-doped_carbon_materials_N-doped_carbon_result_20240812024151.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/MSA-N-CNTs_Ni,_Co,_NiCo,_CoFe,_NiPt_result_20240812030728.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/cobalt_and_nitrogen_codoped_three-dimensional_graphene_Co-N-doped_3D_graphene_result_20240812053550.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Nitrogen_doped_graphene_oxide_nanoribbons_CNx-GONRs_result_20240812033217.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Nitrogen_doped_carbon_nanotubes_NCNTs_result_20240812064116.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/vertically_aligned_carbon_nanotube_arrays_VACNTs_result_20240811175755.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Cobalt_Oxide_Co3O4_result_20240812054430.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Carbon_Nanotubes_CNTs_result_20240812030345.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/nitrogen-doped_carbon_nanotubes_N-CNTs_result_20240812024654.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Fe-doped_CoO_nanotubes_Fe-doped_CoO_NTs_result_20240812073308.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Carbon_Nitride_CN_result_20240812034202.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Nitrogen-doped_carbon_nanotubes_NCNTs_result_20240811183715.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Cobalt_Cerium_Oxide_Co3–xCexO4_result_20240812063148.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/urchin-like_Co3O4_Co3O4_result_20240812054314.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Nitrogen-doped_Carbon_Nanotubes_NCNTs_result_20240811183125.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Co3O4_CoO_heterophase_Co3O4_CoO_result_20240812055237.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240811170427.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/bamboo-like_carbon_nanotube_Fe3C_nanoparticle_hybrid_CNT_Fe3C_result_20240812032433.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/vertically_aligned,_nitrogen-doped_carbon_nanotube_VA-NCNT_result_20240811174323.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/CNT_CNS_architecture_CNT_CNS_result_20240811180454.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Ce_single-atom-doped_Co3O4_nanosheet_Ce-Co3O4_result_20240812043641.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Cobalt-Iron_Oxide_Nanoarrays_CoFe-based_oxide_result_20240812052358.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/nickel_single-atom_electrode_NiI_result_20240812030604.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/Graphitic_Carbon_Nitride_GCN_result_20240812081036.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/vertically_aligned_nitrogen-containing_carbon_nanotubes_VA-NCNTs_result_20240626132728.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/vertically_aligned_nitrogen-containing_carbon_nanotubes_VA-NCNTs_result_20240626133820.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/Buckminsterfullerene_adsorbed_onto_single-walled_carbon_nanotubes_C60-SWCNT_result_20240627131547.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240807121417.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240807125101.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240807124012.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240627005715.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240807131637.json\n",
      "Error decoding JSON in file: /Users/yangz/Documents/projects/llm4catalyst/results/test/Nitrogen-doped_graphene_N-graphene_result_20240626142827.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/Nitrogen-doped_graphene_N-graphene_result_20240626142128.json\n",
      "Error decoding JSON in file: /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240626212537.json\n",
      "Error decoding JSON in file: /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240626202829.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/C60-SWCNT_electrocatalysts_C60-SWCNT_result_20240626005948.json\n",
      "Error decoding JSON in file: /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240626213909.json\n",
      "Error decoding JSON in file: /Users/yangz/Documents/projects/llm4catalyst/results/test/plasma-engraved_Co3O4_nanosheets_Co3O4_result_20240626143558.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/Buckminsterfullerene_adsorbed_onto_single-walled_carbon_nanotubes_C60-SWCNT_result_20240627131547 2.json\n",
      "Processed file: /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240627005715 2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_spectroscopy_data(json_data, spectroscopy_types):\n",
    "    materials = json_data['materials']\n",
    "    new_materials = []\n",
    "\n",
    "    for material in materials:\n",
    "        combined_spectra = {spec_type: defaultdict(list) for spec_type in spectroscopy_types}\n",
    "\n",
    "        for prop in material['properties']:\n",
    "            if prop['type'] in spectroscopy_types:\n",
    "                # Check if the necessary keys exist before accessing them\n",
    "                if 'value' in prop and 'unit' in prop and 'conditions' in prop and 'evidence' in prop:\n",
    "                    combined_spectra[prop['type']]['values'].append(prop['value'])\n",
    "                    combined_spectra[prop['type']]['units'].append(prop['unit'])\n",
    "                    combined_spectra[prop['type']]['conditions'].append(prop['conditions'])\n",
    "                    combined_spectra[prop['type']]['evidence'].append(prop['evidence'])\n",
    "\n",
    "        for spec_type, data in combined_spectra.items():\n",
    "            if data['values']:\n",
    "                combined_vector = {\n",
    "                    \"type\": spec_type,\n",
    "                    \"values\": data['values'],\n",
    "                    \"units\": list(set(data['units'])),\n",
    "                    \"conditions\": list(set(data['conditions'])),\n",
    "                    \"evidence\": sum(data['evidence']) / len(data['evidence']) if data['evidence'] else None\n",
    "                }\n",
    "\n",
    "                material['properties'] = [prop for prop in material['properties'] if prop['type'] != spec_type]\n",
    "                material['properties'].append(combined_vector)\n",
    "\n",
    "        new_materials.append(material)\n",
    "\n",
    "    json_data['materials'] = new_materials\n",
    "    return json_data\n",
    "\n",
    "def process_all_files_in_directory(directory, spectroscopy_types):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        json_data = json.load(f)\n",
    "\n",
    "                    processed_data = process_spectroscopy_data(json_data, spectroscopy_types)\n",
    "\n",
    "                    with open(file_path, 'w') as f:\n",
    "                        json.dump(processed_data, f, indent=4)\n",
    "                    print(f\"Processed file: {file_path}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error decoding JSON in file: {file_path}\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"Missing expected key {e} in file: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while processing file {file_path}: {e}\")\n",
    "\n",
    "# 示例用法\n",
    "spectroscopy_types = [\"XPS\", \"XRD\", \"FTIR\", \"Raman\"]  # 添加你想要处理的其他谱图类型\n",
    "directory = '/Users/yangz/Documents/projects/llm4catalyst/results/'\n",
    "\n",
    "process_all_files_in_directory(directory, spectroscopy_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file /Users/yangz/Documents/projects/llm4catalyst/results/test/Nitrogen-doped_graphene_N-graphene_result_20240626142827.json due to JSON decoding error: Extra data: line 58 column 1 (char 1873)\n",
      "Skipping file /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240626212537.json due to JSON decoding error: Extra data: line 163 column 1 (char 4654)\n",
      "Skipping file /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240626202829.json due to JSON decoding error: Extra data: line 127 column 1 (char 3506)\n",
      "Skipping file /Users/yangz/Documents/projects/llm4catalyst/results/test/nitrogen-doped_graphene_quantum_dots_N-GQDs_result_20240626213909.json due to JSON decoding error: Extra data: line 187 column 1 (char 5349)\n",
      "Skipping file /Users/yangz/Documents/projects/llm4catalyst/results/test/plasma-engraved_Co3O4_nanosheets_Co3O4_result_20240626143558.json due to JSON decoding error: Extra data: line 109 column 1 (char 3199)\n",
      "All data has been written to /Users/yangz/Documents/projects/llm4catalyst/results/combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "def json_to_csv(directory, output_csv_file):\n",
    "    all_keys = set()\n",
    "    rows = []\n",
    "\n",
    "    # First pass: collect all possible property types\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        json_data = json.load(f)\n",
    "                        title = json_data.get('meta', {}).get('title', '')\n",
    "\n",
    "                        for material in json_data.get('materials', []):\n",
    "                            material_name = material.get('material_name', '')\n",
    "                            row = {\n",
    "                                'title': title,\n",
    "                                'material_name': material_name\n",
    "                            }\n",
    "\n",
    "                            for prop in material.get('properties', []):\n",
    "                                prop_type = prop.get('type', '')\n",
    "                                value = prop.get('value', '')\n",
    "                                conditions = prop.get('conditions', '')\n",
    "                                combined_value = f\"{value} ({conditions})\"\n",
    "                                row[prop_type] = combined_value\n",
    "                                all_keys.add(prop_type)\n",
    "\n",
    "                            rows.append(row)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Skipping file {file_path} due to JSON decoding error: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "    # Ensure 'title' and 'material_name' are in the columns\n",
    "    all_keys = sorted(all_keys)\n",
    "    all_keys.insert(0, 'material_name')\n",
    "    all_keys.insert(0, 'title')\n",
    "\n",
    "    # Second pass: write the data to the CSV file\n",
    "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=all_keys)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"All data has been written to {output_csv_file}\")\n",
    "\n",
    "# Example usage\n",
    "directory = '/Users/yangz/Documents/projects/llm4catalyst/results/'\n",
    "output_csv_file = '/Users/yangz/Documents/projects/llm4catalyst/results/combined_data.csv'\n",
    "\n",
    "json_to_csv(directory, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
