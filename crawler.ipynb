{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_element(element):\n",
    "    \"\"\"Extracts all text from a given WebElement, ignoring HTML tags.\"\"\"\n",
    "    return element.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jacs_article(url):\n",
    "    # Initialize WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    # Locate the title element using the updated XPath\n",
    "    title_xpath = '//*[@id=\"pb-page-content\"]/div/main/article/div[4]/div[1]/div[1]/div/div/h1/span'\n",
    "    title_element = driver.find_element(By.XPATH, title_xpath)\n",
    "    title_text = title_element.text\n",
    "    \n",
    "    # Locate the abstract element using the specified XPath\n",
    "    abstract_xpath = '//*[@id=\"pb-page-content\"]/div/main/article/div[4]/div[1]/div[2]'\n",
    "    abstract_element = driver.find_element(By.XPATH, abstract_xpath)\n",
    "    abstract_text = abstract_element.text\n",
    "    \n",
    "    # Locate the main content element using the specified XPath\n",
    "    main_content_xpath = '//*[@id=\"pb-page-content\"]/div/main/article/div[4]/div[1]/div[3]/div[1]/div'\n",
    "    main_content_element = driver.find_element(By.XPATH, main_content_xpath)\n",
    "    main_text = main_content_element.text\n",
    "    \n",
    "    driver.quit()\n",
    "    #print(title_text, abstract_text, main_text)\n",
    "    # Return the extracted details\n",
    "    return  title_text, abstract_text, main_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naturecatalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "def scrape_nature_catalysis_article(url):\n",
    "    \"\"\"Scrape article from Nature website.\"\"\"\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        # Navigate to the specified URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Explicit wait for the title element to be present\n",
    "        title_xpath = '//h1[@class=\"c-article-title\"]'\n",
    "        title_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, title_xpath))\n",
    "        )\n",
    "        title = title_element.text\n",
    "\n",
    "        # Explicit wait for the abstract element to be present\n",
    "        abstract_xpath = '//div[@id=\"Abs1-content\"]'\n",
    "        abstract_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, abstract_xpath))\n",
    "        )\n",
    "        abstract = abstract_element.text\n",
    "\n",
    "        # Explicit wait for the main content element to be present\n",
    "        main_content_xpath = '//*[@id=\"content\"]/main/article/div[2]/div[2]'\n",
    "        main_content_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, main_content_xpath))\n",
    "        )\n",
    "        maintext = main_content_element.text\n",
    "        #print(title, abstract, maintext)\n",
    "        # Return the extracted details \n",
    "        return title, abstract, maintext\n",
    "        \n",
    "        \n",
    "    except NoSuchElementException as e:\n",
    "        print(\"Error: An element was not found on the page.\", str(e))\n",
    "        return None\n",
    "    except TimeoutException as e:\n",
    "        print(\"Error: An element was not found within the given time frame.\", str(e))\n",
    "        return None\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angewandte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def scrape_angewandte_article(url):\n",
    "#     \"\"\"Scrape article from Angewandte website.\"\"\"\n",
    "#     # Initialize Chrome WebDriver\n",
    "#     driver = webdriver.Chrome()\n",
    "    \n",
    "#     try:\n",
    "#         # Navigate to the specified URL\n",
    "#         driver.get(url)\n",
    "\n",
    "#         # Locate the title using the specified XPath\n",
    "#         title_element = driver.find_element(By.XPATH, '//*[@id=\"article__content\"]/div[2]/div/h1')\n",
    "#         title = title_element.text\n",
    "\n",
    "#         # Clean the title: Replace spaces with underscores, remove special characters, and truncate to 20 characters\n",
    "#         clean_title = re.sub(r'[^\\w\\s]', '', title).replace(' ', '_')[:20]\n",
    "        \n",
    "#         # Locate the abstract using its class-based XPath\n",
    "#         abstract_element = driver.find_element(By.XPATH, '//h2[@class=\"article-section__header section__title main abstractlang_en main\"]/following-sibling::div')\n",
    "#         abstract = abstract_element.text\n",
    "        \n",
    "#         # Locate the full text using the specified XPath\n",
    "#         full_text_element = driver.find_element(By.XPATH, '//*[@id=\"article__content\"]')\n",
    "#         maintext = full_text_element.text\n",
    "        \n",
    "#         return title, abstract, maintext\n",
    "    \n",
    "#     except NoSuchElementException:\n",
    "#         print(\"Error: One of the elements was not found on the page.\")\n",
    "#         return None, None, None\n",
    "#     finally:\n",
    "#         # Close the WebDriver\n",
    "#         driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def scrape_angewandte_article(url):\n",
    "    \"\"\"Scrape article from Angewandte website.\"\"\"\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Navigate to the specified URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the title element to be present\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"article__content\"]/div[2]/div/h1'))\n",
    "        )\n",
    "        \n",
    "        # Extract the title using the provided XPath\n",
    "        title_xpath = '//*[@id=\"article__content\"]/div[2]/div/h1'\n",
    "        title_element = driver.find_element(By.XPATH, title_xpath)\n",
    "        title = title_element.text\n",
    "\n",
    "        # Extract the abstract using the provided XPath\n",
    "        abstract_xpath = '//*[@id=\"section-1-en\"]/div'\n",
    "        abstract_element = driver.find_element(By.XPATH, abstract_xpath)\n",
    "        abstract = abstract_element.text\n",
    "        \n",
    "        # Extract the full text using the provided XPath\n",
    "        full_text_xpath = '//*[@id=\"article__content\"]'\n",
    "        full_text_element = driver.find_element(By.XPATH, full_text_xpath)\n",
    "        full_text = full_text_element.text\n",
    "        \n",
    "        # print(f\"Title: {title}\")\n",
    "        # print(f\"Abstract: {abstract}\")\n",
    "        # print(f\"Main Text: {full_text[:100]}...\")\n",
    "\n",
    "        return title, abstract, full_text\n",
    "    \n",
    "    except NoSuchElementException as e:\n",
    "        print(\"Error: An element was not found on the page.\", str(e))\n",
    "        return None, None, None\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "def scrape_science_article(url):\n",
    "    \"\"\"Scrape article from Science website.\"\"\"\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        # Navigate to the specified URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Explicit wait for the title element to be present\n",
    "        title_xpath = '//*[@id=\"main\"]/div[1]/article/header/div/h1'\n",
    "        title_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, title_xpath))\n",
    "        )\n",
    "        title = title_element.text\n",
    "\n",
    "        # Explicit wait for the abstract element to be present\n",
    "        abstract_xpath = '//*[@id=\"abstract\"]'\n",
    "        abstract_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, abstract_xpath))\n",
    "        )\n",
    "        abstract = abstract_element.text\n",
    "\n",
    "        # Explicit wait for the main content element to be present\n",
    "        fulltext_xpath = '//*[@id=\"bodymatter\"]'\n",
    "        fulltext_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, fulltext_xpath))\n",
    "        )\n",
    "        fulltext = fulltext_element.text\n",
    "\n",
    "        # Return the extracted details\n",
    "        return  title,  abstract, fulltext\n",
    "        \n",
    "        \n",
    "    except NoSuchElementException as e:\n",
    "        print(\"Error: An element was not found on the page.\", str(e))\n",
    "        return None\n",
    "    except TimeoutException as e:\n",
    "        print(\"Error: An element was not found within the given time frame.\", str(e))\n",
    "        return None\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science Advances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def scrape_science_advances_article(url):\n",
    "    \"\"\"Scrape article from Science Advances website.\"\"\"\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Navigate to the specified URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the title element to be present\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"main\"]/div[1]/article/header/div/h1'))\n",
    "        )\n",
    "        \n",
    "        # Extract the title using the provided XPath\n",
    "        title_xpath = '//*[@id=\"main\"]/div[1]/article/header/div/h1'\n",
    "        title_element = driver.find_element(By.XPATH, title_xpath)\n",
    "        title = title_element.text\n",
    "\n",
    "        # Extract the abstract using the provided XPath\n",
    "        abstract_xpath = '//*[@id=\"abstract\"]'\n",
    "        abstract_element = driver.find_element(By.XPATH, abstract_xpath)\n",
    "        abstract = abstract_element.text\n",
    "        \n",
    "        # Extract the full text using the provided XPath\n",
    "        full_text_xpath = '//*[@id=\"bodymatter\"]/div'\n",
    "        full_text_element = driver.find_element(By.XPATH, full_text_xpath)\n",
    "        full_text = full_text_element.text\n",
    "        \n",
    "        # Return the extracted details\n",
    "        return title, abstract, full_text\n",
    "    \n",
    "    except NoSuchElementException as e:\n",
    "        print(\"Error: An element was not found on the page.\", str(e))\n",
    "        return None, None, None\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACSNano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def scrape_acsnano_article(url):\n",
    "    \"\"\"Scrape article from ACS Nano website.\"\"\"\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        # Navigate to the specified URL\n",
    "        driver.get(url)\n",
    "\n",
    "        # Extract the title using the provided XPath\n",
    "        title_xpath = '//*[@id=\"pb-page-content\"]/div/main/article/div[4]/div[1]/div[1]/div/div/h1/span'\n",
    "        title_element = driver.find_element(By.XPATH, title_xpath)\n",
    "        title = title_element.text\n",
    "\n",
    "        # Extract the abstract using the provided XPath\n",
    "        abstract_xpath = '//*[@id=\"pb-page-content\"]/div/main/article/div[4]/div[1]/div[2]'\n",
    "        abstract_element = driver.find_element(By.XPATH, abstract_xpath)\n",
    "        abstract = abstract_element.text\n",
    "        \n",
    "        # Locate the full text element using the provided XPath\n",
    "        full_text_xpath = '//*[@id=\"pb-page-content\"]/div/main/article/div[4]/div[1]/div[3]'\n",
    "        full_text_element = driver.find_element(By.XPATH, full_text_xpath)\n",
    "        full_text = full_text_element.text\n",
    "        \n",
    "        # Return the extracted details\n",
    "        return title, abstract, full_text\n",
    "    \n",
    "    except NoSuchElementException as e:\n",
    "        print(\"Error: An element was not found on the page.\", str(e))\n",
    "        return None, None, None\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def scrape_advanced_materials_article(url):\n",
    "    \"\"\"Scrape article from Advanced Materials website.\"\"\"\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Navigate to the specified URL\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the title element to be present\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"article__content\"]/div[2]/div/h1'))\n",
    "        )\n",
    "        \n",
    "        # Extract the title using the provided XPath\n",
    "        title_xpath = '//*[@id=\"article__content\"]/div[2]/div/h1'\n",
    "        title_element = driver.find_element(By.XPATH, title_xpath)\n",
    "        title = title_element.text\n",
    "\n",
    "        # Extract the abstract using the provided XPath\n",
    "        abstract_xpath = '//*[@id=\"article__content\"]/div[5]/article/div[1]'\n",
    "        abstract_element = driver.find_element(By.XPATH, abstract_xpath)\n",
    "        abstract = abstract_element.text\n",
    "        \n",
    "        # Extract the full text using the provided XPath\n",
    "        full_text_xpath = '//*[@id=\"article__content\"]/div[5]/article/section'\n",
    "        full_text_element = driver.find_element(By.XPATH, full_text_xpath)\n",
    "        full_text = full_text_element.text\n",
    "        \n",
    "        # Return the extracted details\n",
    "        return title, abstract, full_text\n",
    "    \n",
    "    except NoSuchElementException as e:\n",
    "        print(\"Error: An element was not found on the page.\", str(e))\n",
    "        return None, None, None\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage\n",
    "# url = 'https://onlinelibrary.wiley.com/doi/full/10.1002/adma.201606793'\n",
    "# title, abstract, full_text = scrape_advanced_materials_article(url)\n",
    "# print(f\"Title: {title}\")\n",
    "# print(f\"Abstract: {abstract}\")\n",
    "# print(f\"Main Text: {full_text[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JMCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_jmca_article(url):\n",
    "    \"\"\"Scrape article from the Journal of Materials Chemistry A (JMCA) website.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    \n",
    "    service = Service()  # Using default ChromeDriver\n",
    "    \n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)  # Set page load timeout\n",
    "\n",
    "    try:\n",
    "        print(f\"Attempting to navigate to URL: {url}\")\n",
    "        driver.get(url)\n",
    "        print(f\"Successfully navigated to URL: {url}\")\n",
    "        \n",
    "        # Wait for the body to be present\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        print(\"Body element found\")\n",
    "        \n",
    "        # Extract the title from the <title> tag\n",
    "        title = driver.title\n",
    "        if title:\n",
    "            title = title.split(' - ')[0].strip()\n",
    "            print(f\"Title extracted: {title}\")\n",
    "        else:\n",
    "            print(\"Could not extract title from <title> tag\")\n",
    "        \n",
    "        # # Extract the abstract\n",
    "        # abstract_xpath = '//*[@id=\"wrapper\"]/div[5]'\n",
    "        # try:\n",
    "        #     abstract_element = WebDriverWait(driver, 20).until(\n",
    "        #         EC.presence_of_element_located((By.XPATH, abstract_xpath))\n",
    "        #     )\n",
    "        #     abstract = abstract_element.text\n",
    "        #     print(\"Abstract extracted successfully\")\n",
    "        # except TimeoutException:\n",
    "        #     abstract = \"Abstract could not be extracted\"\n",
    "        #     print(\"Failed to extract abstract\")\n",
    "        \n",
    "        # Extract text from <h2> elements and their corresponding content\n",
    "        article_content = {}\n",
    "        h2_elements = driver.find_elements(By.TAG_NAME, \"h2\")\n",
    "        #print(f\"Found {len(h2_elements)} h2 elements\")\n",
    "        \n",
    "        for index, h2 in enumerate(h2_elements):\n",
    "            section_title = h2.text\n",
    "            #print(f\"Processing section: {section_title}\")\n",
    "            section_content = \"\"\n",
    "            try:\n",
    "                next_element = h2.find_element(By.XPATH, \"following-sibling::*\")\n",
    "                while next_element.tag_name != \"h2\":\n",
    "                    if next_element.tag_name in [\"p\", \"h3\", \"h4\", \"ul\", \"ol\"]:\n",
    "                        section_content += next_element.text + \"\\n\"\n",
    "                    try:\n",
    "                        next_element = next_element.find_element(By.XPATH, \"following-sibling::*\")\n",
    "                    except NoSuchElementException:\n",
    "                        print(f\"Reached end of content for section: {section_title}\")\n",
    "                        break\n",
    "            except NoSuchElementException:\n",
    "                print(f\"No content found for section: {section_title}\")\n",
    "            \n",
    "            article_content[section_title] = section_content.strip()\n",
    "            print(f\"Finished processing section {index + 1} of {len(h2_elements)}\")\n",
    "        \n",
    "        if not article_content:\n",
    "           # print(\"Article content extracted successfully\")\n",
    "        \n",
    "            print(\"Failed to extract article content\")\n",
    "        if 'Introduction' in article_content:\n",
    "            abstract = article_content['Introduction']\n",
    "        elif '1. Introduction' in article_content:\n",
    "            abstract = article_content['1. Introduction']\n",
    "        else:\n",
    "            abstract = None\n",
    "        # convert json to str\n",
    "        article_content = str(article_content)\n",
    "        return title, abstract, article_content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None, None, None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage\n",
    "# url_jmca = 'https://pubs.rsc.org/en/content/articlelanding/2015/ta/c4ta07214f'\n",
    "# title, abstract, article_content = scrape_jmca_article(url_jmca)\n",
    "# print(f\"\\nFinal Results:\")\n",
    "# print(f\"Title: {title}\")\n",
    "# print(f\"Abstract: {abstract[:100]}...\")  # Print first 100 characters of abstract\n",
    "# print(\"Article Content:\", article_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elsevier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_elsevier_article(url):\n",
    "    \"\"\"Scrape article from Elsevier journal website.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    # chrome_options.add_argument(\"--disable-gpu\")\n",
    "    # chrome_options.add_argument(\"--no-sandbox\")\n",
    "    # chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    # chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "    \n",
    "    service = Service()  # Using default ChromeDriver\n",
    "    \n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.set_page_load_timeout(30)  # Set page load timeout\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        # # Debug: Print out the current page URL and HTML\n",
    "        # print(f\"Currently on URL: {driver.current_url}\")\n",
    "        # print(\"Page HTML:\\n\", driver.page_source[:2000])  # Print the first 2000 characters of the page's HTML\n",
    "        \n",
    "        # Iterate over iframes and try to find the content\n",
    "        iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        print(f\"Number of iframes found: {len(iframes)}\")\n",
    "        \n",
    "        found = False\n",
    "        for i, iframe in enumerate(iframes):\n",
    "            driver.switch_to.frame(iframe)\n",
    "            try:\n",
    "                # Try finding the title in this iframe\n",
    "                title_element = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"title-text\"))\n",
    "                )\n",
    "                title = title_element.text.strip()\n",
    "                #print(f\"Title found in iframe {i}: {title}\")\n",
    "                \n",
    "                # Try finding the abstract in this iframe\n",
    "                abstract_element = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.abstract p\"))\n",
    "                )\n",
    "                abstract = abstract_element.text.strip()\n",
    "                #print(f\"Abstract found in iframe {i}: {abstract[:100]}...\")  # Print the first 100 characters of the abstract\n",
    "                \n",
    "                # If both title and abstract are found, consider the content as found\n",
    "                found = True\n",
    "                break\n",
    "            except (NoSuchElementException, TimeoutException):\n",
    "                print(f\"Content not found in iframe {i}. Moving to the next iframe.\")\n",
    "            finally:\n",
    "                driver.switch_to.default_content()  # Switch back to the main content\n",
    "                \n",
    "        if not found:\n",
    "            print(\"Content not found in any iframe. Checking main content.\")\n",
    "\n",
    "            # Retry in main content if no iframe worked\n",
    "            title_element = WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"title-text\"))\n",
    "            )\n",
    "            title = title_element.text.strip()\n",
    "            #print(f\"Title found: {title}\")\n",
    "            \n",
    "            abstract_element = WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.abstract p\"))\n",
    "            )\n",
    "            abstract = abstract_element.text.strip()\n",
    "            #print(f\"Abstract found: {abstract[:100]}...\")  # Print the first 100 characters of the abstract\n",
    "\n",
    "        # Extract all text from the article's main body by finding all <p> tags within the body\n",
    "        body_element = driver.find_element(By.TAG_NAME, \"body\")\n",
    "        paragraphs = body_element.find_elements(By.TAG_NAME, \"p\")\n",
    "        full_text = \"\\n\".join([p.text for p in paragraphs if p.text.strip()])  # Join all paragraph texts\n",
    "\n",
    "        return title, abstract, full_text\n",
    "\n",
    "    except (NoSuchElementException, TimeoutException) as e:\n",
    "        print(f\"An element was not found or took too long to load: {str(e)}\")\n",
    "        return None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None, None, None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage\n",
    "# url_elsevier = 'https://www.sciencedirect.com/science/article/pii/S0360319920342749'\n",
    "# title, abstract, full_text = scrape_elsevier_article(url_elsevier)\n",
    "# print(f\"\\nFinal Results:\")\n",
    "# print(f\"Title: {title}\")\n",
    "# print(f\"Abstract: {abstract[:100]}...\")  # Print first 100 characters of abstract\n",
    "# print(f\"Full Text: {full_text[:500]}...\")  # Print first 500 characters of full text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Clean the title to 20 characters, removing special symbols.\"\"\"\n",
    "    cleaned_title = re.sub(r'[^\\w\\s]', '', title)  # Remove special symbols\n",
    "    cleaned_title = cleaned_title[:20]  # Limit to 20 characters\n",
    "    return cleaned_title\n",
    "\n",
    "\n",
    "def save_to_file(filename, content):\n",
    "    \"\"\"Save content to a file.\"\"\"\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def determine_journal_from_url(url):\n",
    "    print(f\"check url: {url}\")\n",
    "    if \"acscatal\" in url:\n",
    "        return \"ACS Catalysis\"\n",
    "    elif \"ange\" in url:\n",
    "        return \"Angewandte Chemie\"\n",
    "    elif \"10.1002\" in url or \"adma\" in url:\n",
    "        return \"Advanced Materials\"\n",
    "    elif \"jacs\" in url:\n",
    "        return \"Journal of the American Chemical Society\"\n",
    "    elif \"pubs.acs.org\" in url and (\"nn\" or \"acsnano\") in url:\n",
    "        return \"ACS Nano\"\n",
    "    elif \"10.1021\" in url:\n",
    "        return \"Journal of the American Chemical Society\"\n",
    "    elif '10.1126' in url:\n",
    "        return \"Science\"\n",
    "    elif '10.1126' in url and 'sciadv' in url:\n",
    "        return \"Science Advances\"\n",
    "    elif '/ta/' in url and 'pubs.rsc.org' in url:\n",
    "        return \"Journal of Materials Chemistry A\"\n",
    "    elif 'sciencedirect' in url and 'pii' in url:\n",
    "        return \"Elsevier\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "def search_scholar_and_scrape_related(query, num_related=10):\n",
    "    # 初始化 WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        # 打开 Google Scholar\n",
    "        driver.get('https://scholar.google.com/')\n",
    "        \n",
    "        # 输入搜索查询并搜索\n",
    "        search_box = driver.find_element(By.NAME, 'q')\n",
    "        search_box.send_keys(query)\n",
    "        search_box.submit()\n",
    "\n",
    "        # 等待搜索结果加载\n",
    "        search_results_xpath = '//h3[@class=\"gs_rt\"]/a'\n",
    "        search_results = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, search_results_xpath))\n",
    "        )\n",
    "        \n",
    "        # 查找第一个结果的“Related articles”链接\n",
    "        related_articles_xpath = '//a[contains(@href, \"related:\") and contains(text(), \"相关文章\")]'\n",
    "        related_articles_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, related_articles_xpath))\n",
    "        )\n",
    "        related_articles_element.click()\n",
    "\n",
    "        # 等待相关文章页面加载\n",
    "        time.sleep(5)\n",
    "\n",
    "        # 获取相关的文章链接\n",
    "        length = min(len(driver.find_elements(By.XPATH, '//h3[@class=\"gs_rt\"]/a')), num_related)\n",
    "        related_article_links = driver.find_elements(By.XPATH, '//h3[@class=\"gs_rt\"]/a')[:length]\n",
    "        related_urls = [link.get_attribute('href') for link in related_article_links]\n",
    "        \n",
    "        print(\"Related URLs:\")\n",
    "        for url in related_urls:\n",
    "            print(url)\n",
    "\n",
    "        return related_urls\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "def scrape_article(url, journal):\n",
    "    \n",
    "    \"\"\"Scrape article based on the journal type.\"\"\"\n",
    "    if journal.lower() == 'naturecatalysis' or journal.lower() == 'nature catalysis':\n",
    "        title, abstract, maintext = scrape_nature_catalysis_article(url)\n",
    "    elif journal.lower() == 'jacs' or journal.lower() == 'journal of the american chemical society':\n",
    "        title, abstract, maintext = scrape_jacs_article(url)\n",
    "    elif journal.lower() == 'angewandte' or journal.lower() == 'angewandte chemie':\n",
    "        title, abstract, maintext = scrape_angewandte_article(url)\n",
    "    elif journal.lower() == 'science':\n",
    "        title, abstract, maintext = scrape_science_article(url)\n",
    "    elif journal.lower() == 'acsnano' or journal.lower() == 'acs nano':\n",
    "        title, abstract, maintext = scrape_acsnano_article(url)\n",
    "    elif journal.lower() == 'scienceadvances' or journal.lower() == 'science advances':\n",
    "        title, abstract, maintext = scrape_science_advances_article(url)\n",
    "    elif journal.lower() == 'advancedmaterials' or journal.lower() == 'advanced materials':\n",
    "        title, abstract, maintext = scrape_advanced_materials_article(url)\n",
    "    elif journal.lower() == 'journal of materials chemistry a':\n",
    "        title, abstract, maintext = scrape_jmca_article(url)\n",
    "    elif journal.lower() == 'elsevier':\n",
    "        title, abstract, maintext = scrape_elsevier_article(url)\n",
    "    else:\n",
    "        print(f\"Journal '{journal}' is not supported.\")\n",
    "        return\n",
    "\n",
    "    print(f\"scrapping {url} from {journal}\")\n",
    "    if title and abstract and maintext:\n",
    "        clean_title = re.sub(r'[^\\w\\s]', '', title).replace(' ', '_')[:50]\n",
    "\n",
    "        # print(f\"Title: {title}\")\n",
    "        # print(f\"Abstract: {abstract}\")\n",
    "        # print(f\"Main Text: {maintext[:100]}...\")\n",
    "\n",
    "        # Ensure the documents directory exists\n",
    "        documents_dir = os.path.join(os.getcwd(), 'documents')\n",
    "        abstract_dir = os.path.join(documents_dir, 'abstract')\n",
    "        maintext_dir = os.path.join(documents_dir, 'maintext')\n",
    "        \n",
    "        os.makedirs(abstract_dir, exist_ok=True)\n",
    "        os.makedirs(maintext_dir, exist_ok=True)\n",
    "        \n",
    "        # Save abstract to a file\n",
    "        abstract_filename = os.path.join(abstract_dir, f\"{clean_title}_abs.txt\")\n",
    "        with open(abstract_filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(abstract)\n",
    "            print(f\"Abstract saved to {abstract_filename}\")\n",
    "\n",
    "        # Save maintext to a file\n",
    "        maintext_filename = os.path.join(maintext_dir, f\"{clean_title}_maintext.txt\")\n",
    "        with open(maintext_filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(maintext)\n",
    "            print(f\"Main text saved to {maintext_filename}\")\n",
    "        \n",
    "        # Record the title and url in record.csv\n",
    "        record_filename = os.path.join(documents_dir, 'record.csv')\n",
    "        record_exists = os.path.isfile(record_filename)\n",
    "        \n",
    "        with open(record_filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['title', 'url']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            if not record_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            writer.writerow({'title': clean_title, 'url': url})\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to scrape {url} from {journal}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related URLs:\n",
      "https://www.sciencedirect.com/science/article/pii/S0022286017300625\n",
      "https://www.sciencedirect.com/science/article/pii/S002223131400516X\n",
      "https://www.sciencedirect.com/science/article/pii/S1010603015302380\n",
      "https://www.sciencedirect.com/science/article/pii/S0022231314007121\n",
      "https://europepmc.org/article/med/27209728\n",
      "https://www.sciencedirect.com/science/article/pii/S1386142507002600\n",
      "https://www.sciencedirect.com/science/article/pii/S1386142516300063\n",
      "https://www.sciencedirect.com/science/article/pii/S0003267012015917\n",
      "https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/bio.2625\n",
      "https://www.sciencedirect.com/science/article/pii/S0143720822008385\n",
      "Related URLs: ['https://www.sciencedirect.com/science/article/pii/S0022286017300625', 'https://www.sciencedirect.com/science/article/pii/S002223131400516X', 'https://www.sciencedirect.com/science/article/pii/S1010603015302380', 'https://www.sciencedirect.com/science/article/pii/S0022231314007121', 'https://europepmc.org/article/med/27209728', 'https://www.sciencedirect.com/science/article/pii/S1386142507002600', 'https://www.sciencedirect.com/science/article/pii/S1386142516300063', 'https://www.sciencedirect.com/science/article/pii/S0003267012015917', 'https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/bio.2625', 'https://www.sciencedirect.com/science/article/pii/S0143720822008385']\n",
      "check url: https://www.sciencedirect.com/science/article/pii/S0022286017300625\n",
      "Journal: Elsevier\n",
      "Number of iframes found: 5\n",
      "Content not found in iframe 0. Moving to the next iframe.\n",
      "An unexpected error occurred: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=127.0.6533.120); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x00000001012f50b8 cxxbridge1$str$ptr + 1887276\n",
      "1   chromedriver                        0x00000001012ed794 cxxbridge1$str$ptr + 1856264\n",
      "2   chromedriver                        0x0000000100efc82c cxxbridge1$string$len + 88524\n",
      "3   chromedriver                        0x0000000100f0a224 cxxbridge1$string$len + 144324\n",
      "4   chromedriver                        0x0000000100f02004 cxxbridge1$string$len + 111012\n",
      "5   chromedriver                        0x0000000100f00868 cxxbridge1$string$len + 104968\n",
      "6   chromedriver                        0x0000000100f0305c cxxbridge1$string$len + 115196\n",
      "7   chromedriver                        0x0000000100f030d4 cxxbridge1$string$len + 115316\n",
      "8   chromedriver                        0x0000000100f4235c cxxbridge1$string$len + 374012\n",
      "9   chromedriver                        0x0000000100f79c0c cxxbridge1$string$len + 601516\n",
      "10  chromedriver                        0x0000000100f7848c cxxbridge1$string$len + 595500\n",
      "11  chromedriver                        0x0000000100f35474 cxxbridge1$string$len + 321044\n",
      "12  chromedriver                        0x0000000100f360e4 cxxbridge1$string$len + 324228\n",
      "13  chromedriver                        0x00000001012bca9c cxxbridge1$str$ptr + 1656336\n",
      "14  chromedriver                        0x00000001012c14f8 cxxbridge1$str$ptr + 1675372\n",
      "15  chromedriver                        0x00000001012a2980 cxxbridge1$str$ptr + 1549556\n",
      "16  chromedriver                        0x00000001012c1ca8 cxxbridge1$str$ptr + 1677340\n",
      "17  chromedriver                        0x0000000101294690 cxxbridge1$str$ptr + 1491460\n",
      "18  chromedriver                        0x00000001012deaf0 cxxbridge1$str$ptr + 1795684\n",
      "19  chromedriver                        0x00000001012dec6c cxxbridge1$str$ptr + 1796064\n",
      "20  chromedriver                        0x00000001012ed3c8 cxxbridge1$str$ptr + 1855292\n",
      "21  libsystem_pthread.dylib             0x000000019675df94 _pthread_start + 136\n",
      "22  libsystem_pthread.dylib             0x0000000196758d34 thread_start + 8\n",
      "\n",
      "scrapping https://www.sciencedirect.com/science/article/pii/S0022286017300625 from Elsevier\n",
      "Failed to scrape https://www.sciencedirect.com/science/article/pii/S0022286017300625 from Elsevier\n",
      "check url: https://www.sciencedirect.com/science/article/pii/S002223131400516X\n",
      "Journal: Elsevier\n",
      "Number of iframes found: 5\n",
      "Content not found in iframe 0. Moving to the next iframe.\n",
      "Content not found in iframe 1. Moving to the next iframe.\n",
      "Content not found in iframe 2. Moving to the next iframe.\n",
      "Content not found in iframe 3. Moving to the next iframe.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJournal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjournal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Scrape the article and only add to visited and queue if successful\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_article\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelated_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjournal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m visited\u001b[38;5;241m.\u001b[39madd(related_url) \n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m     55\u001b[0m      \u001b[38;5;66;03m# Add to visited after successful processing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 102\u001b[0m, in \u001b[0;36mscrape_article\u001b[0;34m(url, journal)\u001b[0m\n\u001b[1;32m    100\u001b[0m     title, abstract, maintext \u001b[38;5;241m=\u001b[39m scrape_jmca_article(url)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m journal\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melsevier\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     title, abstract, maintext \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_elsevier_article\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJournal \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjournal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m, in \u001b[0;36mscrape_elsevier_article\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     28\u001b[0m driver\u001b[38;5;241m.\u001b[39mswitch_to\u001b[38;5;241m.\u001b[39mframe(iframe)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Try finding the title in this iframe\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     title_element \u001b[38;5;241m=\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle-text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     title \u001b[38;5;241m=\u001b[39m title_element\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#print(f\"Title found in iframe {i}: {title}\")\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Try finding the abstract in this iframe\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/causal2/lib/python3.9/site-packages/selenium/webdriver/support/wait.py:102\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    100\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 102\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_path = \"documents/record.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(csv_path):\n",
    "    # If the file doesn't exist, create it with the specified headers\n",
    "    df = pd.DataFrame(columns=['title', 'url'])\n",
    "    df.to_csv(csv_path, index=False)\n",
    "else:\n",
    "    # If the file exists, read it\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "initial_urls = [\n",
    "    'https://www.sciencedirect.com/science/article/pii/S0022286017300625'\n",
    "   #'https://www.sciencedirect.com/science/article/pii/S0008622324007619'\n",
    "     #'https://www.science.org/doi/full/10.1126/sciadv.1501122',\n",
    "     # 'https://pubs.acs.org/doi/full/10.1021/nn901850u',\n",
    "    # 'https://www.science.org/doi/full/10.1126/science.1168049',\n",
    "    # 'https://onlinelibrary.wiley.com/doi/full/10.1002/ange.201600687',\n",
    "    # 'https://pubs.acs.org/doi/10.1021/jacs.9b05006',\n",
    "    # 'https://pubs.acs.org/doi/10.1021/acsnano.0c10165',\n",
    "    # 'https://onlinelibrary.wiley.com/doi/full/10.1002/adma.202004670'\n",
    "]\n",
    "\n",
    "visited = set(df['url'].tolist())\n",
    "queue = deque(initial_urls)\n",
    "total_processed = 0\n",
    "max_total = 1000\n",
    "\n",
    "while queue and total_processed < max_total:\n",
    "    initial_url = queue.popleft()\n",
    "    \n",
    "    try:\n",
    "        # Skip the URL if it has already been visited\n",
    "\n",
    "        \n",
    "        # Attempt to find and scrape related URLs\n",
    "        related_urls = search_scholar_and_scrape_related(initial_url, 40)\n",
    "        print(f\"Related URLs: {related_urls}\")\n",
    "        for related_url in related_urls:\n",
    "            # Process each related URL only if it hasn't been visited yet\n",
    "            \n",
    "            if related_url not in visited and total_processed < max_total:\n",
    "                journal = determine_journal_from_url(related_url)\n",
    "                print(f\"Journal: {journal}\")\n",
    "                \n",
    "                # Scrape the article and only add to visited and queue if successful\n",
    "                success = scrape_article(related_url, journal)\n",
    "                visited.add(related_url) \n",
    "                if success:\n",
    "                     # Add to visited after successful processing\n",
    "                    queue.append(related_url)\n",
    "                    total_processed += 1\n",
    "\n",
    "                if total_processed >= max_total:\n",
    "                    print(\"Reached the limit of 100 URLs. Exiting.\")\n",
    "                    break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {initial_url}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Total URLs processed: {total_processed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
